{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import FB15k_237\n",
    "from torch_geometric.nn import ComplEx, DistMult, RotatE, TransE\n",
    "\n",
    "model_map = {\n",
    "    'transe': TransE,\n",
    "    'complex': ComplEx,\n",
    "    'distmult': DistMult,\n",
    "    'rotate': RotatE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying Dataset\n",
    "class mod_FB15k_237(InMemoryDataset):\n",
    "    \n",
    "    url = ('https://raw.githubusercontent.com/villmow/'\n",
    "           'datasets_knowledge_embedding/master/FB15k-237')\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        split: str = \"train\",\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "        force_reload: bool = False,\n",
    "    ):\n",
    "        super().__init__(root, transform, pre_transform, force_reload)\n",
    "\n",
    "        if split not in {'train', 'val', 'test'}:\n",
    "            raise ValueError(f\"Invalid 'split' argument (got {split})\")\n",
    "\n",
    "        path = self.processed_paths[['train', 'val', 'test'].index(split)]\n",
    "        self.load(path)\n",
    "        self.main_embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        self.embedding_dim = 384\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return ['train.txt', 'valid.txt', 'test.txt']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> List[str]:\n",
    "        return ['train_data.pt', 'val_data.pt', 'test_data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        for filename in self.raw_file_names:\n",
    "            download_url(f'{self.url}/{filename}', self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        main_embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        data_list, node_dict, rel_dict = [], {}, {}\n",
    "        for path in self.raw_paths:\n",
    "            with open(path, 'r') as f:\n",
    "                data = [x.split('\\t') for x in f.read().split('\\n')[:-1]]\n",
    "                \n",
    "            edge_index = torch.empty((2, len(data)), dtype=torch.long)\n",
    "            edge_type = torch.empty(len(data), dtype=torch.long)\n",
    "            for i, (src, rel, dst) in enumerate(data):\n",
    "                if src not in node_dict:\n",
    "                    node_dict[src] = len(node_dict)\n",
    "                if dst not in node_dict:\n",
    "                    node_dict[dst] = len(node_dict)\n",
    "                if rel not in rel_dict:\n",
    "                    rel_dict[rel] = len(rel_dict)\n",
    "\n",
    "                edge_index[0, i] = node_dict[src]\n",
    "                edge_index[1, i] = node_dict[dst]\n",
    "                edge_type[i] = rel_dict[rel]\n",
    "\n",
    "            total_nodes = len(node_dict)\n",
    "            data_embedding = torch.empty((total_nodes, 384), dtype=torch.float32)\n",
    "            \n",
    "            for key, value in node_dict.items():\n",
    "                data_embedding[value] = torch.from_numpy(main_embedder.encode(key))\n",
    "\n",
    "            data = Data(x=data_embedding, edge_index=edge_index, edge_type=edge_type)\n",
    "            data_list.append(data)\n",
    "\n",
    "        for data, path in zip(data_list, self.processed_paths):\n",
    "            data.num_nodes = len(node_dict)\n",
    "            self.save([data], path)\n",
    "        \n",
    "        torch.save(node_dict, osp.join(self.processed_dir, 'node_dict.pt'))\n",
    "        torch.save(rel_dict, osp.join(self.processed_dir, 'rel_dict.pt'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class argss():\n",
    "    model = None\n",
    "\n",
    "args = argss()\n",
    "args.model = \"complex\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "path = os.path.join(os.getcwd(), 'data', 'FB15k')\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'FB15k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_data = mod_FB15k_237(path, split='train')[0].to(device)\n",
    "val_data = mod_FB15k_237(path, split='val')[0].to(device)\n",
    "test_data = mod_FB15k_237(path, split='test')[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arg_map = {'rotate': {'margin': 9.0}}\n",
    "model = model_map[args.model](\n",
    "    num_nodes=train_data.num_nodes,\n",
    "    num_relations=train_data.num_edge_types,\n",
    "    hidden_channels=50,\n",
    "    **model_arg_map.get(args.model, {}),\n",
    ").to(device)\n",
    "\n",
    "loader = model.loader(\n",
    "    head_index=train_data.edge_index[0],\n",
    "    rel_type=train_data.edge_type,\n",
    "    tail_index=train_data.edge_index[1],\n",
    "    batch_size=1000,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "optimizer_map = {\n",
    "    'transe': optim.Adam(model.parameters(), lr=0.01),\n",
    "    'complex': optim.Adagrad(model.parameters(), lr=0.1, weight_decay=1e-6),\n",
    "    'distmult': optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6),\n",
    "    'rotate': optim.Adam(model.parameters(), lr=1e-3),\n",
    "}\n",
    "optimizer = optimizer_map[args.model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = total_examples = 0\n",
    "    for head_index, rel_type, tail_index in loader:\n",
    "        # print(head_index.shape, rel_type.shape, tail_index.shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(head_index, rel_type, tail_index)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * head_index.numel()\n",
    "        total_examples += head_index.numel()\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    return model.test(\n",
    "        head_index=data.edge_index[0],\n",
    "        rel_type=data.edge_type,\n",
    "        tail_index=data.edge_index[1],\n",
    "        batch_size=20000,\n",
    "        k=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6276\n",
      "Epoch: 002, Loss: 0.4168\n",
      "Epoch: 003, Loss: 0.3894\n",
      "Epoch: 004, Loss: 0.3585\n",
      "Epoch: 005, Loss: 0.3182\n",
      "Epoch: 006, Loss: 0.2581\n",
      "Epoch: 007, Loss: 0.2032\n",
      "Epoch: 008, Loss: 0.1626\n",
      "Epoch: 009, Loss: 0.1310\n",
      "Epoch: 010, Loss: 0.1086\n",
      "Epoch: 011, Loss: 0.0926\n",
      "Epoch: 012, Loss: 0.0831\n",
      "Epoch: 013, Loss: 0.0737\n",
      "Epoch: 014, Loss: 0.0671\n",
      "Epoch: 015, Loss: 0.0600\n",
      "Epoch: 016, Loss: 0.0602\n",
      "Epoch: 017, Loss: 0.0551\n",
      "Epoch: 018, Loss: 0.0543\n",
      "Epoch: 019, Loss: 0.0506\n",
      "Epoch: 020, Loss: 0.0493\n",
      "Epoch: 021, Loss: 0.0464\n",
      "Epoch: 022, Loss: 0.0473\n",
      "Epoch: 023, Loss: 0.0442\n",
      "Epoch: 024, Loss: 0.0459\n",
      "Epoch: 025, Loss: 0.0444\n",
      "Epoch: 026, Loss: 0.0424\n",
      "Epoch: 027, Loss: 0.0429\n",
      "Epoch: 028, Loss: 0.0420\n",
      "Epoch: 029, Loss: 0.0400\n",
      "Epoch: 030, Loss: 0.0366\n",
      "Epoch: 031, Loss: 0.0373\n",
      "Epoch: 032, Loss: 0.0388\n",
      "Epoch: 033, Loss: 0.0375\n",
      "Epoch: 034, Loss: 0.0372\n",
      "Epoch: 035, Loss: 0.0362\n",
      "Epoch: 036, Loss: 0.0325\n",
      "Epoch: 037, Loss: 0.0371\n",
      "Epoch: 038, Loss: 0.0357\n",
      "Epoch: 039, Loss: 0.0331\n",
      "Epoch: 040, Loss: 0.0347\n",
      "Epoch: 041, Loss: 0.0374\n",
      "Epoch: 042, Loss: 0.0335\n",
      "Epoch: 043, Loss: 0.0354\n",
      "Epoch: 044, Loss: 0.0345\n",
      "Epoch: 045, Loss: 0.0334\n",
      "Epoch: 046, Loss: 0.0345\n",
      "Epoch: 047, Loss: 0.0331\n",
      "Epoch: 048, Loss: 0.0317\n",
      "Epoch: 049, Loss: 0.0338\n",
      "Epoch: 050, Loss: 0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19108/19108 [00:31<00:00, 597.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Val Mean Rank: 4.97, Val Hits@10: 0.9500\n",
      "Epoch: 051, Loss: 0.0334\n",
      "Epoch: 052, Loss: 0.0324\n",
      "Epoch: 053, Loss: 0.0311\n",
      "Epoch: 054, Loss: 0.0308\n",
      "Epoch: 055, Loss: 0.0321\n",
      "Epoch: 056, Loss: 0.0317\n",
      "Epoch: 057, Loss: 0.0300\n",
      "Epoch: 058, Loss: 0.0316\n",
      "Epoch: 059, Loss: 0.0301\n",
      "Epoch: 060, Loss: 0.0296\n",
      "Epoch: 061, Loss: 0.0299\n",
      "Epoch: 062, Loss: 0.0317\n",
      "Epoch: 063, Loss: 0.0317\n",
      "Epoch: 064, Loss: 0.0315\n",
      "Epoch: 065, Loss: 0.0297\n",
      "Epoch: 066, Loss: 0.0297\n",
      "Epoch: 067, Loss: 0.0279\n",
      "Epoch: 068, Loss: 0.0319\n",
      "Epoch: 069, Loss: 0.0315\n",
      "Epoch: 070, Loss: 0.0282\n",
      "Epoch: 071, Loss: 0.0295\n",
      "Epoch: 072, Loss: 0.0308\n",
      "Epoch: 073, Loss: 0.0290\n",
      "Epoch: 074, Loss: 0.0291\n",
      "Epoch: 075, Loss: 0.0296\n",
      "Epoch: 076, Loss: 0.0306\n",
      "Epoch: 077, Loss: 0.0308\n",
      "Epoch: 078, Loss: 0.0275\n",
      "Epoch: 079, Loss: 0.0320\n",
      "Epoch: 080, Loss: 0.0337\n",
      "Epoch: 081, Loss: 0.0273\n",
      "Epoch: 082, Loss: 0.0275\n",
      "Epoch: 083, Loss: 0.0307\n",
      "Epoch: 084, Loss: 0.0277\n",
      "Epoch: 085, Loss: 0.0259\n",
      "Epoch: 086, Loss: 0.0270\n",
      "Epoch: 087, Loss: 0.0289\n",
      "Epoch: 088, Loss: 0.0291\n",
      "Epoch: 089, Loss: 0.0292\n",
      "Epoch: 090, Loss: 0.0292\n",
      "Epoch: 091, Loss: 0.0274\n",
      "Epoch: 092, Loss: 0.0285\n",
      "Epoch: 093, Loss: 0.0276\n",
      "Epoch: 094, Loss: 0.0257\n",
      "Epoch: 095, Loss: 0.0264\n",
      "Epoch: 096, Loss: 0.0282\n",
      "Epoch: 097, Loss: 0.0280\n",
      "Epoch: 098, Loss: 0.0297\n",
      "Epoch: 099, Loss: 0.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5001/5001 [00:08<00:00, 568.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Rank: 6.12, Test Hits@10: 0.9506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    if epoch % 50 == 0:\n",
    "        rank, hits = test(val_data)\n",
    "        print(f'Epoch: {epoch:03d}, Val Mean Rank: {rank:.2f}, Val Hits@10: {hits:.4f}')\n",
    "\n",
    "rank, hits_at_10 = test(test_data)\n",
    "print(f'Test Mean Rank: {rank:.2f}, Test Hits@10: {hits_at_10:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_map[args.model](\n",
    "    num_nodes=train_data.num_nodes,\n",
    "    num_relations=train_data.num_edge_types,\n",
    "    hidden_channels=50,\n",
    "    **model_arg_map.get(args.model, {}),\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load('the_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1366, -0.2797,  0.2002,  ...,  0.2830,  0.2746,  0.1097],\n",
       "        [ 0.0814, -0.5896, -0.8962,  ..., -0.9033, -0.2560,  0.1560],\n",
       "        [-0.0145,  0.1948, -1.0108,  ..., -0.6701, -0.2001,  0.6217],\n",
       "        ...,\n",
       "        [ 0.3291,  0.2613,  0.3307,  ...,  0.9440,  0.5645,  0.1645],\n",
       "        [ 0.4018, -0.0290, -0.1692,  ..., -0.3449, -0.0832,  0.0210],\n",
       "        [-0.1087,  0.1511,  0.0817,  ...,  0.1513,  0.1277, -0.2447]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.node_emb_im.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(10, 50)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rel_emb_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"the_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_f(data, i):\n",
    "    model.eval()\n",
    "    return model.forward(\n",
    "        head_index=data.edge_index[0][i],\n",
    "        rel_type=data.edge_type[i],\n",
    "        tail_index=data.edge_index[1][i]\n",
    "    ), model.forward(\n",
    "        head_index=torch.tensor(377),\n",
    "        rel_type=data.edge_type[i],\n",
    "        tail_index=data.edge_index[1][i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.3295), tensor(-9.5511))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f(test_data, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "query_embedder = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Sentences we want to encode. Example:\n",
    "query = 'This framework generates embeddings for each input sentence'\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embedding = query_embedder.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = embedding.shape[0]\n",
    "outs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1003,  0.0943,  0.1052,  0.1153,  0.0044,  0.0219,  0.1041, -0.1143,\n",
      "          0.1274,  0.0653, -0.2182,  0.0231, -0.2135,  0.0518,  0.2047, -0.1126,\n",
      "         -0.1226, -0.0461, -0.1216,  0.0396, -0.0251,  0.1064, -0.1649, -0.1477,\n",
      "         -0.1437,  0.0428, -0.0475, -0.0475, -0.0539, -0.1179, -0.1762,  0.0820,\n",
      "         -0.0650, -0.0501,  0.2219,  0.1215, -0.1297,  0.2061, -0.0109, -0.0877,\n",
      "          0.1786,  0.0603,  0.1462, -0.0615,  0.0046, -0.0199,  0.1151,  0.2076,\n",
      "          0.0327, -0.0273]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QuestionMLP(nn.Module):\n",
    "    def __init__(self, ins, outs):\n",
    "        super(QuestionMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(ins, 128)  # First linear layer\n",
    "        self.linear2 = nn.Linear(128, 64)   # Second linear layer\n",
    "        self.linear3 = nn.Linear(64, 32)    # Third linear layer\n",
    "        self.linear4 = nn.Linear(32, 2*outs)  # Fourth linear layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))  # Apply ReLU activation function\n",
    "        x = F.relu(self.linear2(x))  # Apply ReLU activation function\n",
    "        x = F.relu(self.linear3(x))  # Apply ReLU activation function\n",
    "        x = self.linear4(x)          # No activation after last layer\n",
    "        return x\n",
    "\n",
    "model = QuestionMLP(ins, outs)\n",
    "\n",
    "# Example input\n",
    "input_tensor = torch.randn(1, ins)  # Batch size of 1\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class God(nn.Module):\n",
    "    def __init__(self, ins, outs):\n",
    "        super(God, self).__init__()\n",
    "        self.query_embedder = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.qmodel = QuestionMLP(ins, outs)\n",
    "        \n",
    "        self.kgmodel = model_map[args.model](\n",
    "            num_nodes=train_data.num_nodes,\n",
    "            num_relations=train_data.num_edge_types,\n",
    "            hidden_channels=50,\n",
    "            **model_arg_map.get(args.model, {}),\n",
    "        ).to(device)\n",
    "        self.kgmodel.load_state_dict(torch.load('the_model.pt'))\n",
    "\n",
    "        self.entity_emb_im = self.kgmodel.node_emb_im.weight\n",
    "        self.relation_emb_im = self.kgmodel.rel_emb_im.weight\n",
    "        self.entity_emb_re = self.kgmodel.node_emb.weight\n",
    "        self.relation_emb_re = self.kgmodel.rel_emb.weight\n",
    "\n",
    "        self.entity_dict = torch.load(osp.join(\"\", 'node_dict.pt'))\n",
    "        self.relation_dict = torch.load(osp.join(\"\", 'rel_dict.pt'))\n",
    "\n",
    "\n",
    "    def forward(self, question, relation, entity):\n",
    "        qx = self.query_embedder.encode(question)\n",
    "        qx = self.qmodel(qx)\n",
    "        qx_re = qx[:outs]\n",
    "        qx_im = qx[outs:]\n",
    "\n",
    "        rx = self.relation_emb[self.relation_dict[relation]]\n",
    "        ex = self.entity_emb[self.entity_dict[entity]]\n",
    "\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'In which year was the movie \"Interstellar\" with Matthew McConaughey released?',\n",
       "  'answer': '2014',\n",
       "  'entities': ['Interstellar']},\n",
       " {'question': 'Identify a 2015 film starring Jennifer Lawrence.',\n",
       "  'answer': 'The Hunger Games: Mockingjay - Part 2',\n",
       "  'entities': ['2015', 'Jennifer Lawrence']},\n",
       " {'question': 'Name a movie released in 2013 with Christian Bale in the lead role.',\n",
       "  'answer': 'American Hustle',\n",
       "  'entities': ['2013', 'Christian Bale']}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "file_path = \"data/dataset.txt\"\n",
    "\n",
    "# Function to process the dataset\n",
    "def process_dataset(file_path):\n",
    "\n",
    "    all_data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize lists to hold the columns of our dataframe\n",
    "    questions = []\n",
    "    entity_1 = []\n",
    "    entity_2 = []\n",
    "    answers = []\n",
    "\n",
    "    # Regular expressions for identifying entities in the questions\n",
    "    year_regex = r\"\\b(19\\d{2}|20\\d{2})\\b\"\n",
    "    movie_regex = r'\\\"([^\\\"]+)\\\"'  # Matches movie titles in quotes\n",
    "\n",
    "    # Process each line\n",
    "    for i in range(0, len(lines), 3):  # Each question-answer pair is separated by an empty line\n",
    "        data = {}\n",
    "        question = lines[i].strip()\n",
    "        answer = lines[i+1].split(\":\", 1)[1].strip()  # Splitting by ':' and taking the second part\n",
    "        data['question'] = question\n",
    "        data['answer'] = answer\n",
    "\n",
    "        # Extract entities from the question\n",
    "        years = re.findall(year_regex, question)\n",
    "        movies = re.findall(movie_regex, question)\n",
    "        names = re.sub(year_regex, '', question)  # Remove years\n",
    "        names = re.sub(movie_regex, '', names)  # Remove movie titles\n",
    "        names = re.findall(r'\\b[A-Z][a-z]*\\s[A-Z][a-z]*\\b', names)  # Match proper names\n",
    "\n",
    "        data['entities'] = []\n",
    "        data['entities'].extend(years)\n",
    "        data['entities'].extend(movies)\n",
    "        data['entities'].extend(names)\n",
    "\n",
    "        # Add the data to the lists\n",
    "        all_data.append(data)\n",
    "    return all_data\n",
    "\n",
    "# Process the dataset\n",
    "main_data = process_dataset(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "main_data[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
